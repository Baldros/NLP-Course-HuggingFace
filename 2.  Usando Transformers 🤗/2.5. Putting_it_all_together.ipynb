{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODXJ91xbAVqhWMqcg0NRbU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Baldros/NLP-Course-HuggingFace/blob/main/2.5.%20Putting_it_all_together.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ApresentaÃ§Ã£o:\n",
        "\n",
        "    O modulo 2 do curso Ã© uma passagem superficial de como funciona\n",
        "    a utilizaÃ§Ã£o dos modelos do ecossistema da Hugging Face, passando\n",
        "    um pouco, para isso, por alguns conceitos importantes da arquitetura\n",
        "    Transformer para NLP. A ideia aqui Ã© juntar tudo que foi visto\n",
        "    de modo a consolidar o conhecimento."
      ],
      "metadata": {
        "id": "txrzHbTyp9Ry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reunindo tudo:\n",
        "\n",
        "    Nas Ãºltimas seÃ§Ãµes, tentamos fazer a maior parte do trabalho\n",
        "    manualmente. Exploramos como os tokenizadores funcionam e\n",
        "    analisamos a tokenizaÃ§Ã£o, conversÃ£o para IDs de entrada,\n",
        "    preenchimento, truncamento e mÃ¡scaras de atenÃ§Ã£o.\n",
        "\n",
        "    No entanto, como vimos na seÃ§Ã£o 2, a API ðŸ¤— Transformers pode\n",
        "    lidar com tudo isso para nÃ³s com uma funÃ§Ã£o de alto nÃ­vel na qual\n",
        "    mergulharemos aqui. Quando vocÃª chama seu tokenizador diretamente\n",
        "    na frase, vocÃª recebe de volta entradas prontas para passar pelo\n",
        "    seu modelo:"
      ],
      "metadata": {
        "id": "lX9YWJtWyjxY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    Essa primeira etapa Ã© igual para os dois casos, tanto para\n",
        "    o Pytorch, quanto para o Tensorflow."
      ],
      "metadata": {
        "id": "nPum1DR8sC5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo o checkpoint:\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "# Definindo a sequencia:\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\""
      ],
      "metadata": {
        "id": "z-AtYsp3q8iP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pytorch**"
      ],
      "metadata": {
        "id": "2r-ImOevrrNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando dependencia:\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "-OJvcLcFrFzp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQmH0VBgpw6E",
        "outputId": "5d32072d-e70a-451c-fdab-3c1238139908"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids:[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\n",
            "attention_mask:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "# Instanciando o tokenizador:\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# tokenizando:\n",
        "model_inputs = tokenizer(sequence)\n",
        "\n",
        "# Visualizando:\n",
        "for key in model_inputs.keys():\n",
        "  print(f'{key}:{model_inputs[key]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "    Aqui, a variÃ¡vel model_inputs contÃ©m tudo o que Ã© necessÃ¡rio para\n",
        "    que um modelo funcione bem. Para o DistilBERT, isso inclui os IDs\n",
        "    de entrada, bem como a mÃ¡scara de atenÃ§Ã£o. Outros modelos que aceitam\n",
        "    entradas adicionais tambÃ©m terÃ£o essas saÃ­das pelo objeto do tokenizador.\n",
        "\n",
        "    Poderiamos tambÃ©m ter uma entrada dupla:"
      ],
      "metadata": {
        "id": "7jWTI4oSrzx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequencia com duas entradas:\n",
        "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
        "\n",
        "# TokenizaÃ§Ã£o:\n",
        "model_inputs = tokenizer(sequences)\n",
        "\n",
        "# Visualizando:\n",
        "for key in model_inputs.keys():\n",
        "  print(f'{key}:{model_inputs[key]} -> ({len(model_inputs[key])}) -> ({len(model_inputs[key][0])},{len(model_inputs[key][1])})')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5AM8QQvq3pn",
        "outputId": "d6366781-e98b-465c-cd8c-f9b7d80b2e4b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids:[[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102]] -> (2) -> (16,6)\n",
            "attention_mask:[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]] -> (2) -> (16,6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos preencher de acordo com diversos objetivos:"
      ],
      "metadata": {
        "id": "SSQOvjZbuVx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preenchendo as sequÃªncias atÃ© o comprimento mÃ¡ximo de sequÃªncia\n",
        "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
        "\n",
        "# Visualizando:\n",
        "for key in model_inputs.keys():\n",
        "  print(f'{key}:{model_inputs[key]} -> ({len(model_inputs[key])}) -> ({len(model_inputs[key][0])},{len(model_inputs[key][1])})')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zusNpCf5tzTt",
        "outputId": "42627ce9-db61-47ff-91b6-81e180aeaae5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids:[[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]] -> (2) -> (16,16)\n",
            "attention_mask:[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]] -> (2) -> (16,16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preenchendo as sequÃªncias atÃ© o comprimento mÃ¡ximo do modelo\n",
        "# (512 para BERT ou DistilBERT)\n",
        "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
        "\n",
        "# Visualizando:\n",
        "for key in model_inputs.keys():\n",
        "  print(f'{key}:{model_inputs[key]} -> ({len(model_inputs[key])}) -> ({len(model_inputs[key][0])},{len(model_inputs[key][1])})')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2lhUzwyuy6k",
        "outputId": "02da0ecd-c2f0-4be3-abd9-1c2aa5656044"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids:[[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2061, 2031, 1045, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]] -> (2) -> (512,512)\n",
            "attention_mask:[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]] -> (2) -> (512,512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preenchendo as sequÃªncias atÃ© o comprimento mÃ¡ximo especificado\n",
        "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)\n",
        "# Visualizando:\n",
        "for key in model_inputs.keys():\n",
        "  print(f'{key}:{model_inputs[key]} -> ({len(model_inputs[key])}) -> ({len(model_inputs[key][0])},{len(model_inputs[key][1])})')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShPe33Cfu2Af",
        "outputId": "e51862c0-eae5-4b2e-c1b2-c9b016d29a61"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids:[[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102, 0, 0]] -> (2) -> (16,8)\n",
            "attention_mask:[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0]] -> (2) -> (16,8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TambÃ©m podemos truncar sequÃªncias:"
      ],
      "metadata": {
        "id": "j7jdP0RLvTED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Truncando as sequÃªncias que forem mais longas do que o comprimento mÃ¡ximo do modelo\n",
        "# (512 for BERT or DistilBERT)\n",
        "model_inputs = tokenizer(sequences, truncation=True)\n",
        "\n",
        "# Visualizando:\n",
        "for key in model_inputs.keys():\n",
        "  print(f'{key}:{model_inputs[key]} -> ({len(model_inputs[key])}) -> ({len(model_inputs[key][0])},{len(model_inputs[key][1])})')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9ZC5munu6BE",
        "outputId": "aa0ae405-35e2-4372-d6af-ef3c7d9d0b9a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids:[[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102]] -> (2) -> (16,6)\n",
            "attention_mask:[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]] -> (2) -> (16,6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# truncar as sequÃªncias que forem mais longas do que o comprimento mÃ¡ximo especificado\n",
        "model_inputs = tokenizer(sequences, max_length=8, truncation=True)\n",
        "\n",
        "# Visualizando:\n",
        "for key in model_inputs.keys():\n",
        "  print(f'{key}:{model_inputs[key]} -> ({len(model_inputs[key])}) -> ({len(model_inputs[key][0])},{len(model_inputs[key][1])})')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OBXonZive-c",
        "outputId": "3a97a69b-fad7-49cb-c8a3-288b0e3fe3d3"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids:[[101, 1045, 1005, 2310, 2042, 3403, 2005, 102], [101, 2061, 2031, 1045, 999, 102]] -> (2) -> (8,6)\n",
            "attention_mask:[[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]] -> (2) -> (8,6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    O objeto do tokenizador pode lidar com a conversÃ£o para tensores\n",
        "    especÃ­ficos do framework, que podem entÃ£o ser enviados diretamente\n",
        "    para o modelo. Por exemplo, no seguinte trecho de cÃ³digo, estamos\n",
        "    solicitando ao tokenizador que retorne tensores dos diferentes\n",
        "    frameworks - \"pt\" retorna tensores PyTorch, \"tf\" retorna tensores\n",
        "    TensorFlow e \"np\" retorna arrays NumPy:"
      ],
      "metadata": {
        "id": "oQOS4oWxwJbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retornando PyTorch tensors\n",
        "model_inputs_pt = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "# Visualizando:\n",
        "for key in model_inputs_pt.keys():\n",
        "  print(f'{key}:{model_inputs_pt[key]} -> ({len(model_inputs_pt[key])}) -> ({len(model_inputs_pt[key][0])},{len(model_inputs_pt[key][1])})\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZWfxWPpwTVa",
        "outputId": "ecd9e8fa-c662-4e1c-9bef-0051bf2a2f1c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids:tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
            "          2607,  2026,  2878,  2166,  1012,   102],\n",
            "        [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0]]) -> (2) -> (16,16)\n",
            "\n",
            "attention_mask:tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]) -> (2) -> (16,16)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retornando TensorFlow tensors\n",
        "model_inputs_tf = tokenizer(sequences, padding=True, return_tensors=\"tf\")\n",
        "\n",
        "# Visualizando:\n",
        "for key in model_inputs_tf.keys():\n",
        "  print(f'{key}:{model_inputs_tf[key]} -> ({len(model_inputs_tf[key])}) -> ({len(model_inputs_tf[key][0])},{len(model_inputs_tf[key][1])})\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdZPk2c9v--F",
        "outputId": "4aca8350-0e57-4b38-f7a2-0878d7205d6c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids:[[  101  1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026\n",
            "   2878  2166  1012   102]\n",
            " [  101  2061  2031  1045   999   102     0     0     0     0     0     0\n",
            "      0     0     0     0]] -> (2) -> (16,16)\n",
            "\n",
            "attention_mask:[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]] -> (2) -> (16,16)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retornando NumPy arrays\n",
        "model_inputs_np = tokenizer(sequences, padding=True, return_tensors=\"np\")\n",
        "\n",
        "# Visualizando:\n",
        "for key in model_inputs_np.keys():\n",
        "  print(f'{key}:{model_inputs_np[key]} -> ({len(model_inputs_np[key])}) -> ({len(model_inputs_np[key][0])},{len(model_inputs_np[key][1])})\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QqUIfRzwZjH",
        "outputId": "1deb3018-23e8-4ede-dafa-1b8e05f74c7f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids:[[  101  1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026\n",
            "   2878  2166  1012   102]\n",
            " [  101  2061  2031  1045   999   102     0     0     0     0     0     0\n",
            "      0     0     0     0]] -> (2) -> (16,16)\n",
            "\n",
            "attention_mask:[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]] -> (2) -> (16,16)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokens especiais**\n",
        "\n",
        "    Se dermos uma olhada nos IDs de entrada retornados pelo tokenizador,\n",
        "    veremos que eles sÃ£o um pouco diferentes do que tÃ­nhamos antes:"
      ],
      "metadata": {
        "id": "SBaO1sHCxK0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizando sequencia:\n",
        "model_inputs = tokenizer(sequence)\n",
        "print(model_inputs[\"input_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PT8ovRcxwp-N",
        "outputId": "afeccad1-ab29-40b6-a80b-4691cc43f528"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\n",
            "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizando:\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "\n",
        "# Convertendo tokens em ids:\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jouwvkd0xSKe",
        "outputId": "0558a863-bc27-4373-8db9-b6b9c3f7ac60"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    Um ID de token foi adicionado no inÃ­cio e outro\n",
        "    no final. Se decodificarmos eles, teremos:"
      ],
      "metadata": {
        "id": "he4QEIQUxloB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checando decodificaÃ§Ã£o:\n",
        "print(tokenizer.decode(model_inputs[\"input_ids\"]))\n",
        "print(tokenizer.decode(ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lw7ZFTAxTnl",
        "outputId": "1c8a59ab-6871-4333-9fb1-8560c5988cf6"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] i've been waiting for a huggingface course my whole life. [SEP]\n",
            "i've been waiting for a huggingface course my whole life.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    Legal que Ã© o mesmo mÃ©todo, mas consegue decodificar as duas\n",
        "    codificaÃ§Ãµes diferentes. Porque Ã© isso que sÃ£o, sÃ£o duas\n",
        "    codificaÃ§Ãµes diferentes, mas relacionadas."
      ],
      "metadata": {
        "id": "tfVQgtdux1h3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checando igualdade:\n",
        "ids == model_inputs[\"input_ids\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mGRITRzx_Dj",
        "outputId": "dba02b5b-6049-43ca-eec1-edcf635c44e5"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "    O tokenizador adicionou a palavra especial [CLS] no inÃ­cio e a\n",
        "    palavra especial [SEP] no final. Isso ocorre porque o modelo foi\n",
        "    prÃ©-treinado com elas, entÃ£o, para obter os mesmos resultados para\n",
        "    inferÃªncia, precisamos adicionÃ¡-las tambÃ©m. Observe que alguns modelos\n",
        "    nÃ£o adicionam palavras especiais, ou adicionam diferentes; os modelos\n",
        "    tambÃ©m podem adicionar essas palavras especiais apenas no inÃ­cio ou\n",
        "    apenas no final. De qualquer forma, o tokenizador sabe quais sÃ£o\n",
        "    esperadas e lidarÃ¡ com isso para vocÃª."
      ],
      "metadata": {
        "id": "u4CM5NdlyL0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resumindo: Do tokenizador para o modelo\n",
        "\n",
        "    Agora que vimos todos os passos individuais que o objeto do\n",
        "    tokenizador usa quando aplicado em textos, vamos ver uma Ãºltima\n",
        "    vez como ele pode lidar com mÃºltiplas sequÃªncias (preenchimento!),\n",
        "    sequÃªncias muito longas (truncamento!), e mÃºltiplos tipos de tensores\n",
        "    com sua API principal:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oKjWlrdIyeZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensorflow**"
      ],
      "metadata": {
        "id": "Rjy_V5sXzTz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependencias utilizadas:\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "coMj3kF0xksm"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciando tokenizador:\n",
        "tokenizer_tf = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# Instanciando modelo:\n",
        "model_tf = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3uMVs6izcsO",
        "outputId": "1b4963f0-fe75-4142-8429-4c89056bff18"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
            "\n",
            "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizando sequÃªncias:\n",
        "tokens_tf = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "\n",
        "# Gerando as saÃ­das:\n",
        "output_tf = model_tf(**tokens_tf);output_tf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsMoqqBezVua",
        "outputId": "699b2881-a099-47b1-88b1-d323e3e701c4"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
              "array([[-1.5606961,  1.6122813],\n",
              "       [-3.618318 ,  3.9137495]], dtype=float32)>, hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pytorch**"
      ],
      "metadata": {
        "id": "O3FVNstAzH1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DependÃªncias utilizadas:\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "M0ezwhvNz1pM"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciando Tokenizador:\n",
        "tokenizer_pt = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# Instanciando modelo:\n",
        "model_pt = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "hGPMZN2Gz4If"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizando sequÃªncias:\n",
        "tokens_pt = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Gerando as saÃ­das:\n",
        "output_pt = model_pt(**tokens_pt);output_pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C_n0WVP0Awe",
        "outputId": "86d87780-5ee3-4297-906a-7dc54dcc95d5"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n",
              "        [-3.6183,  3.9137]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    }
  ]
}
