# Introdu√ß√£o:
No Cap√≠tulo 1, vimos que os modelos Transformer geralmente s√£o muito grandes, com milh√µes a dezenas de bilh√µes de par√¢metros, tornando o treinamento e a implementa√ß√£o desses modelos se torna uma empreitada complicada. Al√©m disso, com novos modelos sendo lan√ßados quase diariamente, cada um com sua pr√≥pria implementa√ß√£o, experiment√°-los n√£o √© uma tarefa f√°cil.

A biblioteca ü§ó Transformers do ecossistema Hugging Face foi criada para resolver esse problema. Seu objetivo √© fornecer uma API √∫nica por meio da qual qualquer modelo Transformer possa ser carregado, treinado e salvo. As principais caracter√≠sticas da biblioteca s√£o:

**1. Facilidade de uso:** Baixar, carregar e usar um modelo de **processamento de linguagem natural (NLP)** de √∫ltima gera√ß√£o para infer√™ncia pode ser feito em apenas duas linhas de c√≥digo.

**2. Flexibilidade:** No n√∫cleo, todos os modelos s√£o classes simples PyTorch nn.Module ou TensorFlow tf.keras.Model e podem ser manipulados como qualquer outro modelo em seus respectivos frameworks de aprendizado de m√°quina (ML). Esse √© um ponto muito importante que deve ser internalizado, afinal de contas, nenhum modelo vem pronto, precisamos ajusta-los a nossas necessidades.

**3. Simplicidade:** Quase nenhuma abstra√ß√£o √© feita em toda a biblioteca. O conceito central √© o "Tudo em um arquivo": a passagem direta de um modelo √© inteiramente definida em um √∫nico arquivo, para que o c√≥digo em si seja compreens√≠vel e modific√°vel.

Essa √∫ltima caracter√≠stica torna a ü§ó Transformers bastante diferente de outras bibliotecas de ML. Os modelos n√£o s√£o constru√≠dos em m√≥dulos compartilhados entre arquivos; em vez disso, cada modelo tem suas pr√≥prias camadas. Al√©m de tornar os modelos mais acess√≠veis e compreens√≠veis, isso permite que voc√™ experimente facilmente em um modelo sem afetar outros. Entender esse funcionamento √© muito importante para a implementa√ß√£o dos modelos que constam no ecossistema Hugging Face. 

## Conteudo do Cap√≠tulo:
Este cap√≠tulo come√ßar√° com um exemplo de ponta a ponta em que usamos um modelo e um **tokenizador** juntos para replicar a fun√ß√£o pipeline() apresentada no Cap√≠tulo 1. Em seguida, discutiremos a API do modelo: mergulharemos nas classes de modelo, configura√ß√£o e mostraremos como carregar um modelo e como ele processa entradas num√©ricas para produzir previs√µes.

Em seguida, examinaremos a **API do tokenizador**, que √© o outro componente principal da fun√ß√£o pipeline(). Os **tokenizadores** cuidam das primeiras e √∫ltimas etapas de processamento, lidando com a **convers√£o de texto para entradas num√©ricas** para a rede neural e a convers√£o de volta para texto quando necess√°rio. Finalmente, mostraremos como lidar com o envio de v√°rias senten√ßas por meio de um modelo em um lote preparado e concluiremos com uma an√°lise mais detalhada da fun√ß√£o de alto n√≠vel tokenizer().
