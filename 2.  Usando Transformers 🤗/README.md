# Introdu√ß√£o:
No Cap√≠tulo 1, voc√™ viu que os modelos Transformer geralmente s√£o muito grandes, com milh√µes a dezenas de bilh√µes de par√¢metros, tornando o treinamento e a implementa√ß√£o desses modelos uma empreitada complicada. Al√©m disso, com novos modelos sendo lan√ßados quase diariamente, cada um com sua pr√≥pria implementa√ß√£o, experiment√°-los todos n√£o √© uma tarefa f√°cil.

A biblioteca ü§ó Transformers foi criada para resolver esse problema. Seu objetivo √© fornecer uma API √∫nica por meio da qual qualquer modelo Transformer possa ser carregado, treinado e salvo. As principais caracter√≠sticas da biblioteca s√£o:

Facilidade de uso: Baixar, carregar e usar um modelo de processamento de linguagem natural (PNL) de √∫ltima gera√ß√£o para infer√™ncia pode ser feito em apenas duas linhas de c√≥digo.

Flexibilidade: No n√∫cleo, todos os modelos s√£o classes simples PyTorch nn.Module ou TensorFlow tf.keras.Model e podem ser manipulados como qualquer outro modelo em seus respectivos frameworks de aprendizado de m√°quina (ML).

Simplicidade: Quase nenhuma abstra√ß√£o √© feita em toda a biblioteca. O conceito central √© o "Tudo em um arquivo": a passagem direta de um modelo √© inteiramente definida em um √∫nico arquivo, para que o c√≥digo em si seja compreens√≠vel e modific√°vel.

Essa √∫ltima caracter√≠stica torna a ü§ó Transformers bastante diferente de outras bibliotecas de ML. Os modelos n√£o s√£o constru√≠dos em m√≥dulos compartilhados entre arquivos; em vez disso, cada modelo tem suas pr√≥prias camadas. Al√©m de tornar os modelos mais acess√≠veis e compreens√≠veis, isso permite que voc√™ experimente facilmente em um modelo sem afetar outros.

Este cap√≠tulo come√ßar√° com um exemplo de ponta a ponta em que usamos um modelo e um tokenizador juntos para replicar a fun√ß√£o pipeline() apresentada no Cap√≠tulo 1. Em seguida, discutiremos a API do modelo: mergulharemos nas classes de modelo e configura√ß√£o e mostraremos como carregar um modelo e como ele processa entradas num√©ricas para produzir previs√µes.

Em seguida, examinaremos a API do tokenizador, que √© o outro componente principal da fun√ß√£o pipeline(). Os tokenizadores cuidam das primeiras e √∫ltimas etapas de processamento, lidando com a convers√£o de texto para entradas num√©ricas para a rede neural e a convers√£o de volta para texto quando necess√°rio. Finalmente, mostraremos como lidar com o envio de v√°rias senten√ßas por meio de um modelo em um lote preparado e concluiremos com uma an√°lise mais detalhada da fun√ß√£o de alto n√≠vel tokenizer().
