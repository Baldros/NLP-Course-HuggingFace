# Modelos Encoder-Only:

Os modelos **Encoder-Only** referem-se a uma categoria de modelos baseados na arquitetura Transformer, que utilizam apenas a parte do encoder do modelo, ou seja, a parte **classificadora** do transformer e são comumente chamados de **modelos de autoencoder**. Esses modelos são caracterizados por terem atenção **bidirecional**, permitindo que as camadas de atenção acessem todas as palavras na sentença original. No pré-treinamento, esses modelos geralmente **corrompem uma sentença**, como mascarar palavras aleatórias, e são treinados para recuperar ou reconstruir a sentença original. Modelos **Encoder-Only** são ideais para tarefas que exigem compreensão de sentenças completas, como classificação de sentenças, reconhecimento de entidades nomeadas e resposta a perguntas extrativas.

## Representantes desta família de modelos incluem:

**1. ALBERT (A Lite BERT):** Uma variação eficiente de BERT que mantém o desempenho em tarefas de NLP (Processamento de Linguagem Natural), mas com uma arquitetura mais leve. É projetado para reduzir o número de parâmetros, tornando-o mais rápido e eficiente.(https://huggingface.co/docs/transformers/model_doc/albert)

**2. BERT (Bidirectional Encoder Representations from Transformers):** Um modelo que revolucionou o campo de NLP com sua atenção bidirecional. BERT é amplamente usado para tarefas como classificação de sentenças, reconhecimento de entidades nomeadas e perguntas e respostas.(https://huggingface.co/docs/transformers/model_doc/bert)

**3. DistilBERT:** Uma versão mais leve de BERT, projetada para ser mais eficiente em termos de computação e recursos, mantendo um desempenho significativo. É adequado para cenários onde a eficiência computacional é crucial.(https://huggingface.co/docs/transformers/model_doc/distilbert)

**4. ELECTRA:** Um modelo que propõe uma abordagem inovadora para treinamento eficiente, substituindo palavras reais por palavras falsas (máscaras) no processo de treinamento. Isso resulta em um modelo mais eficiente sem sacrificar o desempenho.(https://huggingface.co/docs/transformers/model_doc/electra)

**5. RoBERTa (Robustly optimized BERT approach):**  Uma variação aprimorada de BERT, otimizada para maior robustez e desempenho consistente em diferentes tarefas de NLP. Utiliza treinamento mais longo e uma abordagem de máscara dinâmica.(https://huggingface.co/docs/transformers/model_doc/roberta)
