# Como um fransformer funciona?

Redes Neurais (ou neuronais, a depender do autor) é uma técnica de aprendizado de máquina
que foi se refinando ao longo do tempo. Desde os perceptrons de Rosenblatt, desenvolvidos
entre os anos 50 e 60, muita coisa aconteceu, mas poucas coisas foram tão importantes para
a técnica quanto a elaboração dos transformers e o seu mecanismo de atenção. Aqui, o curso
faz uma abordagem teorica sobre a construção do conceito, dando uma uma olhada de alto nível
na arquitetura dos modelos Transformer. 

## Um pouco de história:
**Junho de 2018: GPT**, o primeiro modelo Transformer pré-treinado, usado para ajuste fino em
várias tarefas de PNL e obteve resultados de última geração;
https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf

**Outubro de 2018: BERT**, outro grande modelo pré-treinado, projetado para produzir melhores
resumos de frases (mais sobre isso no próximo capítulo!);
https://arxiv.org/pdf/1810.04805.pdf

**Fevereiro de 2019: GPT-2**, uma versão melhorada (e maior) do GPT que não foi divulgada
publicamente imediatamente devido a questões éticas;
https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf

**Outubro de 2019: DistilBERT**, uma versão destilada do BERT que é 60% mais rápida, 40% mais
leve na memória e ainda mantém 97% do desempenho do BERT;
https://arxiv.org/pdf/1910.01108.pdf

**Outubro de 2019: BART e T5**, dois grandes modelos pré-treinados usando a mesma arquitetura 
do modelo Transformer original (o primeiro a fazê-lo);
Bart: https://arxiv.org/pdf/1910.13461.pdf
T5: https://arxiv.org/pdf/1910.10683.pdf

**Maio de 2020, GPT-3**, uma versão ainda maior do GPT-2 que é capaz de funcionar bem em
uma variedade de tarefas sem a necessidade de ajuste fino (chamado aprendizado zero-shot);

Esta lista está longe de ser abrangente e serve apenas para destacar alguns dos diferentes
tipos de modelos de Transformer. Em termos gerais, eles podem ser agrupados em três categorias:

1. GPT-like (também chamados de modelos de Transformer auto-regressivos)
2. BERT-like (também chamados de modelos Transformer de codificação automática)
3. BART/T5-like (também chamados de modelos de transformador sequência a sequência)

**Nota - Calculo de gasto energético:**

Um ponto interessente comentado no curso é o gasto energético no treinamento de um modelo. Ele oferece uma
forma de calculalo utilizando a biblioteca *codecarbon*. Acho interessante mensionar, inclusive para utilizar
esse tipo de monitoramente como mais um tipo de métrica para o modelo.

## Transferência de aprendizagem:

A ideia da aprendizagem por transferência é explorar o conhecimento exigido por um modelo treinado com muitos dados em outra tarefa.
Ou seja, a ideia é usar um modelo, já treinando para uma tarefa específica, em outra tarefa para alem daquela que ele foi treinando.
Basicamente esse capitulo intruz a ideia de modelos pré-treinando, a base do ecossistema da Hugging Face. 

Este pré-treinamento geralmente é realizado em grandes volumes de dados. Portanto, requer um corpus de dados muito extenso, e o treinamento pode levar várias semanas.

A afinação (Fine Tuning), por outro lado, é o treinamento realizado após um modelo ter sido pré-treinado. Para realizar a afinação, primeiro você adquire um modelo de linguagem pré-treinado e, em seguida, realiza treinamento adicional com um conjunto de dados específico para a sua tarefa. Dito isso, poderiamos pensar em algumas razões para se utilizar modelos pré-treinandos:

1. O modelo pré-treinado já foi treinado em um conjunto de dados que tem algumas semelhanças com o conjunto de dados de afinação. O processo de afinação pode, portanto, aproveitar o conhecimento adquirido pelo modelo inicial durante o pré-treinamento (por exemplo, com problemas de PNL, o modelo pré-treinado terá alguma compreensão estatística da linguagem que você está usando para sua tarefa).

2. Como o modelo pré-treinado já foi treinado em muitos dados, a afinação requer muito menos dados para obter resultados decentes.

3. Por causa disso, a quantidade de tempo e recursos necessários para obter bons resultados é muito menor.

Por exemplo, alguém poderia aproveitar um modelo pré-treinado treinado na língua inglesa e, em seguida, ajustá-lo com um corpus arXiv, resultando em um modelo baseado em ciência/pesquisa. A afinação só exigirá uma quantidade limitada de dados: o conhecimento adquirido pelo modelo pré-treinado é "transferido", daí o termo aprendizado de transferência.

A afinação de um modelo, portanto, tem custos menores em termos de tempo, dados, financeiros e ambientais. Também é mais rápido e mais fácil iterar sobre diferentes esquemas de afinação, pois o treinamento é menos restritivo do que um pré-treinamento completo.

**Problema de Viés:**

Um ponto negativo importante de modelos pré-treinados é que eles carregam também os viéses do modelo. Ou seja, os preconceitos gerados no treinamento
do modelo são carregados para outras aplicações que se queiram fazer com aquele respectivo pré-modelo.

## Arquitetura do Transformer:

Nessa seção, será introduzido a arquitetura geral do modelo Transformer. A ideia aqui, não é esgotar o tema, o conceito de transformers é um conceito denso e será abordados em outras seções que cobriram os detalhes deste tema. 

A arquitetura do Transformer foi originalmente projetada para tradução. Durante o treinamento, o codificador recebe entradas (frases) em um determinado idioma, enquanto o decodificador recebe as mesmas frases no idioma alvo desejado. No codificador, as camadas de atenção podem usar todas as palavras em uma frase (já que, como acabamos de ver, a tradução de uma palavra específica pode depender do que está após e antes dela na frase). O decodificador, no entanto, trabalha sequencialmente e só pode prestar atenção às palavras na frase que já foram traduzidas (ou seja, apenas as palavras antes da palavra atualmente gerada). Por exemplo, quando prevemos as três primeiras palavras do alvo traduzido, as fornecemos ao decodificador, que então usa todas as entradas do codificador para tentar prever a quarta palavra.

Para acelerar as coisas durante o treinamento (quando o modelo tem acesso a frases alvo), o decodificador recebe todo o alvo, mas não pode usar palavras futuras (se tivesse acesso à palavra na posição 2 ao tentar prever a palavra na posição 2, o problema não seria muito difícil!). Por exemplo, ao tentar prever a quarta palavra, a camada de atenção terá acesso apenas às palavras nas posições de 1 a 3.
Dito isso, podemos começar definindo que o um transformer é composto de duas peças, **encoder** e **decoder**, sendo:

**Encoder:** O encoder recebe uma entrada e constrói uma representação dela (suas características). Isso significa que o modelo é otimizado para adquirir compreensão a partir da entrada. O encoder trabalha muito com a ideia de classificação, não atoa modelos encoder-only são tidos como modelos classficadores. O **encoder** aceita entradas que representam texto. Ele converte esse texto, essas **palavras tonkenizadas**, em representações numéricas pelo processo de **embedding**. Essas representações numéricas também podem ser chamadas de **embeddings** ou **características (features)**. Veremos que ele usa o **mecanismo de autoatenção (self-atention)** como seu principal componente, bem como suas **propriedades bidirecionais**.

**Decoder:** O decoder utiliza a representação (características) do codificador juntamente com outras entradas para gerar uma sequência alvo. Isso significa que o modelo é otimizado para gerar saídas. Não atoa são chamados de modelos geradores. O **decoder** é semelhante ao encoder: também pode aceitar as mesmas entradas do encoder, entradas que representam texto. Ele usa um mecanismo semelhante ao do encoder, que também é a **autoatenção mascarada**. A diferença acondete devido à sua propriedade **unidirecional** e é tradicionalmente usada de maneira **auto-regressiva**.

**Encoder-Decoder (Sequence to Sequence):** A combinação das duas partes resulta no que é conhecido como **encoder-decoder** ou transformer **sequence-to-sequence**. O **encoder** aceita entradas e calcula uma representação de alto nível dessas entradas. Essas saídas são então passadas para o **decoder**. O **decoder** usa a saída do codificador juntamente com outras entradas, a fim de gerar uma previsão. Em seguida, ele prevê uma saída, que será reutilizado em iterações futuras, daí o termo **"auto-regressivo"**.
