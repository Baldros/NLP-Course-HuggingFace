# Modelos Encoder-Decoder (Sequence-to-Sequence):

Os modelos **encoder-decoder**, também conhecidos como modelos **sequence-to-sequence**, representam uma categoria especializada de **arquiteturas** que integram ambas as parte da **arquitetura transformer**. Essa abordagem permite que, em cada estágio, as camadas de atenção do **encoder** acessem todas as palavras na **sentença inicial**, enquanto as camadas de atenção do **decoder** podem apenas acessar as palavras posicionadas antes de uma dada palavra na entrada.

No pré-treinamento desses modelos, os objetivos podem derivar tanto de modelos de **encoder** quanto de **decoder**, mas frequentemente envolvem estratégias mais complexas. Por exemplo, a **arquitetura T5** é pré-treinado substituindo trechos aleatórios de texto por uma única palavra especial de máscara, e o objetivo é prever o texto que essa palavra de máscara substitui.

Esses modelos **sequence-to-sequence** são particularmente eficazes em tarefas que exigem a **geração de novas sentenças com base em uma entrada fornecida**. Aplicações notáveis incluem resumos automáticos, tradução de idiomas e respostas generativas a perguntas, onde a capacidade de compreender e sintetizar informações para criar sequências significativas é crucial.

## Representatives of this family of models include:

**1. BART (Bidirectional and Auto-Regressive Transformers):** Projetado para tarefas de geração de texto, BART utiliza uma abordagem bidirecional e autoregressiva. É eficaz em aplicações como resumo de texto, tradução e preenchimento de lacunas. (https://huggingface.co/docs/transformers/model_doc/bart)

**2. mBART (Multilingual BART):** Uma versão multilíngue do BART, mBART é adaptado para trabalhar com vários idiomas. Comumente utilizado em tarefas de tradução automática e geração de texto multilíngue. (https://huggingface.co/docs/transformers/model_doc/mbart)

**3. Marian:** Especializado em tradução automática, o Marian é conhecido por sua eficiência e desempenho em cenários de tradução entre diferentes idiomas. Oferece suporte a uma variedade de pares de idiomas.(https://huggingface.co/docs/transformers/model_doc/marian)

**4. T5 (Text-to-Text Transfer Transformer):** Com uma abordagem text-to-text, o T5 é altamente versátil e pode ser aplicado a uma ampla gama de tarefas de processamento de linguagem natural (PLN). Seu treinamento envolve a substituição de trechos de texto por uma palavra de máscara, com o objetivo de prever o texto original. (https://huggingface.co/docs/transformers/model_doc/t5)
